{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27448e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize spark\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7605ab5-07af-4e59-a6b2-a0166385c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.types as tp\n",
    "from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec\n",
    "from pyspark.ml.feature import Tokenizer as pyTokenizer\n",
    "#from pyspark.ml.feature import RegexTokenizer,StopWordsRemover,CountVectorizer,IDF, HashingTF\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from  pyspark.ml.pipeline import PipelineModel  # For saving the model\n",
    "from pyspark.sql.functions import col\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Johnsnow nlp library\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.common import *\n",
    "from sparknlp.base import *\n",
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bb883a-c88d-47a3-b68e-1e60a80157fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Running spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2caa769",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/bin/spark-submit --master spark://131.114.50.200:7077 kafka-script/tweet_consumer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ff72aacf-00e4-4d93-9b4e-4512f07fd8c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.master.Master, logging to /Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/logs/spark-JoeKifle-org.apache.spark.deploy.master.Master-1-cpe-172-100-10-56.twcny.res.rr.com.out\n"
     ]
    }
   ],
   "source": [
    "!/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/sbin/start-master.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2a6be175-a962-4eec-86fb-5a34f536d7fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting org.apache.spark.deploy.worker.Worker, logging to /Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/logs/spark-JoeKifle-org.apache.spark.deploy.worker.Worker-1-cpe-172-100-10-56.twcny.res.rr.com.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/logs/spark-JoeKifle-org.apache.spark.deploy.worker.Worker-2-cpe-172-100-10-56.twcny.res.rr.com.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/logs/spark-JoeKifle-org.apache.spark.deploy.worker.Worker-3-cpe-172-100-10-56.twcny.res.rr.com.out\n",
      "starting org.apache.spark.deploy.worker.Worker, logging to /Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/logs/spark-JoeKifle-org.apache.spark.deploy.worker.Worker-4-cpe-172-100-10-56.twcny.res.rr.com.out\n"
     ]
    }
   ],
   "source": [
    "!/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/sbin/start-worker.sh spark://cpe-172-100-10-56.twcny.res.rr.com:7077"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4309c66d-25b7-47f0-b404-878681ab4202",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/sbin/stop-worker.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3f129f42-f3ec-4304-8df2-e65c26eecc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/sbin/stop-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e232f8e9-0bfd-40fb-9912-459770b72fff",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5579ff58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/13 11:28:48 WARN Utils: Your hostname, joetelila.local resolves to a loopback address: 127.0.0.1; using 192.168.1.96 instead (on interface en0)\n",
      "22/05/13 11:28:48 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/JoeKifle/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/JoeKifle/.ivy2/jars\n",
      "com.johnsnowlabs.nlp#spark-nlp_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-e1dc11d8-51d1-44e4-83ba-a871b1806b95;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.johnsnowlabs.nlp#spark-nlp_2.12;3.4.4 in central\n",
      "\tfound com.typesafe#config;1.4.2 in central\n",
      "\tfound org.rocksdb#rocksdbjni;6.5.3 in central\n",
      "\tfound com.amazonaws#aws-java-sdk-bundle;1.11.603 in central\n",
      "\tfound com.github.universal-automata#liblevenshtein;3.0.0 in central\n",
      "\tfound com.google.code.findbugs#annotations;3.0.1 in central\n",
      "\tfound net.jcip#jcip-annotations;1.0 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.1 in central\n",
      "\tfound com.google.protobuf#protobuf-java-util;3.0.0-beta-3 in central\n",
      "\tfound com.google.protobuf#protobuf-java;3.0.0-beta-3 in central\n",
      "\tfound com.google.code.gson#gson;2.3 in central\n",
      "\tfound it.unimi.dsi#fastutil;7.0.12 in central\n",
      "\tfound org.projectlombok#lombok;1.16.8 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.21 in central\n",
      "\tfound com.navigamez#greex;1.0 in central\n",
      "\tfound dk.brics.automaton#automaton;1.11-8 in central\n",
      "\tfound org.json4s#json4s-ext_2.12;3.5.3 in central\n",
      "\tfound joda-time#joda-time;2.9.5 in central\n",
      "\tfound org.joda#joda-convert;1.8.1 in central\n",
      "\tfound com.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 in central\n",
      ":: resolution report :: resolve 1231ms :: artifacts dl 34ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazonaws#aws-java-sdk-bundle;1.11.603 from central in [default]\n",
      "\tcom.github.universal-automata#liblevenshtein;3.0.0 from central in [default]\n",
      "\tcom.google.code.findbugs#annotations;3.0.1 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.1 from central in [default]\n",
      "\tcom.google.code.gson#gson;2.3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java;3.0.0-beta-3 from central in [default]\n",
      "\tcom.google.protobuf#protobuf-java-util;3.0.0-beta-3 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#spark-nlp_2.12;3.4.4 from central in [default]\n",
      "\tcom.johnsnowlabs.nlp#tensorflow-cpu_2.12;0.3.3 from central in [default]\n",
      "\tcom.navigamez#greex;1.0 from central in [default]\n",
      "\tcom.typesafe#config;1.4.2 from central in [default]\n",
      "\tdk.brics.automaton#automaton;1.11-8 from central in [default]\n",
      "\tit.unimi.dsi#fastutil;7.0.12 from central in [default]\n",
      "\tjoda-time#joda-time;2.9.5 from central in [default]\n",
      "\tnet.jcip#jcip-annotations;1.0 from central in [default]\n",
      "\torg.joda#joda-convert;1.8.1 from central in [default]\n",
      "\torg.json4s#json4s-ext_2.12;3.5.3 from central in [default]\n",
      "\torg.projectlombok#lombok;1.16.8 from central in [default]\n",
      "\torg.rocksdb#rocksdbjni;6.5.3 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.21 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   20  |   0   |   0   |   0   ||   20  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-e1dc11d8-51d1-44e4-83ba-a871b1806b95\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 20 already retrieved (0kB/19ms)\n",
      "22/05/13 11:28:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# dependency for spark-sql-kafka\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")\n",
    "conf.set(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:3.4.4\")\n",
    "#conf.set(\"spark.cores.max\", \"8\")\n",
    "#conf.set(\"spark.shuffle.service.enabled\", \"true\")\n",
    "#conf.set(\"spark.dynamicAllocation.enabled\", \"true\")\n",
    "\n",
    "\n",
    "#conf.set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1\")\n",
    "spark_master = \"spark://131.114.50.200:7077\"\n",
    "#spark_master = \"spark://joetelila.lan:7077\"\n",
    "#sc = pyspark.SparkContext(master=spark_master,appName=\"Hello Spark\")\n",
    "spark = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(spark_master)\\\n",
    "        .appName(\"sentimentAnalysis\")\\\n",
    "        .config(conf=conf)\\\n",
    "        .getOrCreate()\n",
    "#spark._sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c05f9f-9d50-4cfa-887a-98112b29e4f8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Reading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da860524-7187-4b5f-af04-4bb66e146c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the schema\n",
    "my_schema = tp.StructType([\n",
    "  tp.StructField(name= 'text',       dataType= tp.StringType(),   nullable= True),\n",
    "  tp.StructField(name= 'polarity',    dataType= tp.IntegerType(),  nullable= True)\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f5fbbb1-a6dc-4eaf-95a8-158c5bb54fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/13 11:30:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:31:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:31:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:31:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:31:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py\", line 475, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/opt/anaconda3/envs/pyspark_env/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/JoeKifle/Documents/EDU/Pisa_2020_2021/Semester_4/DEP/Sentiment-analysis-using-kafka-spark/trainingModel.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/JoeKifle/Documents/EDU/Pisa_2020_2021/Semester_4/DEP/Sentiment-analysis-using-kafka-spark/trainingModel.ipynb#ch0000011?line=0'>1</a>\u001b[0m \u001b[39m# read the dataset  \u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/JoeKifle/Documents/EDU/Pisa_2020_2021/Semester_4/DEP/Sentiment-analysis-using-kafka-spark/trainingModel.ipynb#ch0000011?line=1'>2</a>\u001b[0m tweet_data \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39;49mread\u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39mdata/tweets_dataset_may6_no_comma.csv\u001b[39;49m\u001b[39m'\u001b[39;49m,inferSchema\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, header\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py:410\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py?line=407'>408</a>\u001b[0m     path \u001b[39m=\u001b[39m [path]\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py?line=408'>409</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mtype\u001b[39m(path) \u001b[39m==\u001b[39m \u001b[39mlist\u001b[39m:\n\u001b[0;32m--> <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py?line=409'>410</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_df(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jreader\u001b[39m.\u001b[39;49mcsv(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_spark\u001b[39m.\u001b[39;49m_sc\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mPythonUtils\u001b[39m.\u001b[39;49mtoSeq(path)))\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py?line=410'>411</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py?line=411'>412</a>\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1312'>1313</a>\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1314'>1315</a>\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1315'>1316</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1316'>1317</a>\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1317'>1318</a>\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1319'>1320</a>\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1320'>1321</a>\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1321'>1322</a>\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1323'>1324</a>\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1035'>1036</a>\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1036'>1037</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1037'>1038</a>\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1038'>1039</a>\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py?line=1039'>1040</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py?line=472'>473</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py?line=473'>474</a>\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py?line=474'>475</a>\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py?line=475'>476</a>\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py?line=476'>477</a>\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    <a href='file:///Users/JoeKifle/spark-3.2.1-bin-hadoop3.2/python/lib/py4j-0.10.9.3-src.zip/py4j/clientserver.py?line=477'>478</a>\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pyspark_env/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/pyspark_env/lib/python3.8/socket.py?line=666'>667</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/pyspark_env/lib/python3.8/socket.py?line=667'>668</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///opt/anaconda3/envs/pyspark_env/lib/python3.8/socket.py?line=668'>669</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/pyspark_env/lib/python3.8/socket.py?line=669'>670</a>\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    <a href='file:///opt/anaconda3/envs/pyspark_env/lib/python3.8/socket.py?line=670'>671</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/05/13 11:32:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:32:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:32:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:32:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:33:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:33:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:33:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:33:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:34:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:34:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:34:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:34:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:35:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:35:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:35:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:35:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:36:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:36:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:36:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:36:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:37:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:37:15 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:37:30 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:37:45 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "22/05/13 11:38:00 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "# read the dataset  \n",
    "tweet_data = spark.read.csv('data/tweets_dataset_may6_no_comma.csv',inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f37d01-2025-4b8b-b72b-149941927fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing handles and links from the tweets\n",
    "tweet_data = tweet_data.withColumn('text', F.regexp_replace('text','@[A-Za-z0-9_]+',''))\n",
    "tweet_data = tweet_data.withColumn('text', F.regexp_replace('text','https?://[^ ]+',''))\n",
    "tweet_data = tweet_data.withColumn('text', F.regexp_replace('text','www.[^ ]+',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96818e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a1b2c-587c-416f-b0ed-4b49f909163d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the schema of the file\n",
    "tweet_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc7e373-624d-44f0-8536-b107f9f2264c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping null columns\n",
    "tweet_data=tweet_data.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56dff17-f965-4698-a5a0-c4d46e4c9e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show distribution of the polarity\n",
    "tweet_data.groupBy(\"polarity\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a2f757-cd8e-4438-8cf9-85c2bd65e304",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Logistic regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff2b14-56bc-4280-b417-77dc7ba43fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stages For the Pipeline\n",
    "tokenizer = pyTokenizer(inputCol='text',outputCol='mytokens')\n",
    "stopwords_remover = StopWordsRemover(inputCol='mytokens',outputCol='filtered_tokens')\n",
    "word_2_vec = Word2Vec(inputCol= 'filtered_tokens', outputCol= 'w2v', vectorSize=200) #, vectorSize= 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7122fa4d-43a5-4c9c-aa01-baa877e334f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LogisticRegression(featuresCol='vector',labelCol='polarity')\n",
    "model = LogisticRegression(featuresCol= 'w2v',labelCol= 'polarity', regParam=0.008, maxIter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadf85d4-558b-42c8-9d52-846f12c58afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the pipeline\n",
    "pipeline = Pipeline(stages= [tokenizer, stopwords_remover, word_2_vec, model])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e993f6-4d4b-412e-993a-587b78e936d0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eef2b4d-932e-44eb-8e00-7d26c23385cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split Dataset and train\n",
    "(train_tweet,test_tweet) = tweet_data.randomSplit((0.8,0.2),seed=42)\n",
    "pipelineFit = pipeline.fit(train_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299c395d-cc02-4ff8-96b2-91371cb79719",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c1b9d-0183-4578-8a93-4798cda840c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on our - Test Dataset.\n",
    "predictions = pipelineFit.transform(test_tweet)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='polarity',predictionCol='prediction',metricName='f1')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcde62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# persisting the model\n",
    "pipelineFit.write().overwrite().save('pipeline_lr_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdc65e",
   "metadata": {},
   "source": [
    "### 2. Logistric regression with transformation from Johnsnow nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57241fb1",
   "metadata": {},
   "source": [
    "#### Building pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa4abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sparknlp\n",
    "#from pyspark.ml.feature import CountVectorizer, HashingTF, IDF, OneHotEncoder, StringIndexer, VectorAssembler, SQLTransformer\n",
    "\n",
    "document_assembler = DocumentAssembler() \\\n",
    "      .setInputCol(\"text\") \\\n",
    "      .setOutputCol(\"document\")\n",
    "tokenizer = Tokenizer() \\\n",
    "      .setInputCols([\"document\"]) \\\n",
    "      .setOutputCol(\"token\")\n",
    "normalizer = Normalizer() \\\n",
    "      .setInputCols([\"token\"]) \\\n",
    "      .setOutputCol(\"normalized\")\n",
    "stopwords_cleaner = StopWordsCleaner()\\\n",
    "      .setInputCols(\"normalized\")\\\n",
    "      .setOutputCol(\"cleanTokens\")\\\n",
    "      .setCaseSensitive(False)\n",
    "stemmer = Stemmer() \\\n",
    "      .setInputCols([\"cleanTokens\"]) \\\n",
    "      .setOutputCol(\"stem\")\n",
    "finisher = Finisher() \\\n",
    "      .setInputCols([\"stem\"]) \\\n",
    "      .setOutputCols([\"token_features\"]) \\\n",
    "      .setOutputAsArray(True) \\\n",
    "      .setCleanAnnotations(False)\n",
    "countVectors = CountVectorizer(inputCol=\"token_features\", outputCol=\"features\", vocabSize=10000, minDF=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faefff76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the pipeline\n",
    "nlp_pipeline = Pipeline(\n",
    "    stages=[document_assembler, \n",
    "            tokenizer,\n",
    "            normalizer,\n",
    "            stopwords_cleaner, \n",
    "            stemmer, \n",
    "            finisher,\n",
    "            countVectors\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea304280",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model = nlp_pipeline.fit(tweet_data)\n",
    "processed = nlp_model.transform(tweet_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458c33e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.select('text','token_features').show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2ffe2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed.select('text','features','polarity').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb8c04d",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0037bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set seed for reproducibility\n",
    "(trainingData, testData) = processed.randomSplit((0.8,0.2),seed=42)\n",
    "#print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "#print(\"Test Dataset Count: \" + str(testData.count()))\n",
    "\n",
    "lr = LogisticRegression(regParam=0.008, maxIter=10000, labelCol=\"polarity\")\n",
    "lrModel = lr.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755690a6",
   "metadata": {},
   "source": [
    "#### Evaluating model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d9cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = lrModel.transform(testData)\n",
    "# show polarity and prediction columns\n",
    "predictions.select('polarity','prediction').show(20,truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb265e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on our - Test Dataset.\n",
    "#predictions = pipelineFit.transform(test_tweet)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='polarity',predictionCol='prediction',metricName='f1')\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "93d105ada02192d0bbdad0d4b7f9e51b851ebd5be727b9bab0a9679568b88653"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pyspark_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
